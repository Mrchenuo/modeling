{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import interp\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "# 1 加载数据\n",
    "train_X_df = pd.read_excel(r\"D:\\GitHub\\modeling\\D\\ans3\\Molecular_Descriptor.xlsx\", engine='openpyxl',\n",
    "                           sheet_name='training')\n",
    "train_Y_df = pd.read_excel(r\"D:\\GitHub\\modeling\\D\\ans3\\ADMET.xlsx\", engine='openpyxl', sheet_name='training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "var_not_null = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "# # 只用第一问的20个特征\n",
    "# var = ['MDEC-23', 'minsssN', 'LipoaffinityIndex', 'maxHsOH', 'maxssO', 'C1SP2',\n",
    "#                    'minHsOH', 'BCUTc-1l', 'minsOH', 'minHBint5', 'MLFER_A', 'nHBAcc', 'VC-5',\n",
    "#                    'MDEO-12', 'ndssC', 'TopoPSA', 'ATSc3', 'SHBint10', 'MDEC-33', 'XLogP']\n",
    "# var_top20 = pd.Index(var)\n",
    "# # var_top20 = pd.Index(var_top20)\n",
    "# # X_train_df = train_X_df[var_top20]\n",
    "# X_train_df = train_X_df\n",
    "# 找出全零的列，将其剔除\n",
    "# idx_df = pd.read_excel(r\"D:\\GitHub\\modeling\\D\\ans3\\idx_not_null.xlsx\", engine='openpyxl', sheet_name='Sheet1')\n",
    "# idx = idx_df.values.tolist()\n",
    "# idx_one = [b for a in idx for b in a]\n",
    "# X_train_df = train_X_df[idx_one]\n",
    "\n",
    "X = train_X_df.iloc[:, 1:].values  # 去掉第一列\n",
    "Y = train_Y_df.iloc[:, 1].values  # 第一个label\n",
    "\n",
    "\n",
    "# idx_series = pd.Series(idx_df[:,:].values)\n",
    "# df_not_null = X_train_df.loc[:, idx_series]\n",
    "# print(df_not_null.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# print(X_train_df.shape)\n",
    "# idx = (X_train_df != 0).any(axis=0)\n",
    "# # 编码指定为UTF-8否则读取的时候会报错\n",
    "# idx.to_csv(\"idx_not_null.csv\", header='true', encoding='utf-8')\n",
    "# df = X_train_df.loc[:, (X_train_df != 0).any(axis=0)]\n",
    "# # X_train_df = X_train_df.loc[~(X_train_df==0).all(axis=0), :]\n",
    "# print(df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# 2 降维\n",
    "# scikit_kpca = KernelPCA()\n",
    "#\n",
    "# # 使用KPCA降低数据维度，直接获得投影后的坐标\n",
    "# X = scikit_kpca.fit_transform(X)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "# 3 分割训练数据和测试数据\n",
    "# random_state 设置随机数为33\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=33)\n",
    "\n",
    "# 3 在训练之前，我们需要对数据进行规范化，这样让数据同在同一个量级上，避免因为维度问题造成数据误差：\n",
    "# 采用 Z-Score 规范化数据，保证每个特征维度的数据均值为 0，方差为 1\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.transform(x_test)\n",
    "\n",
    "nag_sum = np.sum(y_train==0)\n",
    "pos_sum = np.sum(y_train==1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm准确率:  0.9139240506329114\n",
      "(395, 2)\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# 4 创建 SVM 分类器\n",
    "# model_svm = svm.SVC(kernel='poly',degree=8, gamma='auto')\n",
    "model_svm = svm.SVC(decision_function_shape=\"ovr\",probability=True)\n",
    "# 用训练集做训练\n",
    "model_svm.fit(x_train, y_train)\n",
    "# 用测试集做预测\n",
    "p_svm = model_svm.predict(x_test)\n",
    "print('svm准确率: ', metrics.accuracy_score(p_svm, y_test))\n",
    "test_predict_label = model_svm.predict_proba(x_test)\n",
    "print(test_predict_label.shape)\n",
    "print(model_svm.classes_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def modelfit(alg, x_train, y_train, x_test, y_test, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(x_train, y_train,eval_metric='auc')\n",
    "\n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(x_test)\n",
    "    dtrain_predprob = alg.predict_proba(x_test)[:,1]\n",
    "\n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_test, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y_test, dtrain_predprob))\n",
    "\n",
    "    # feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)[:20]\n",
    "    # feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    # plt.ylabel('Feature Importance Score')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nuo_nuaa\\anaconda3\\envs\\diveintoDL\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9114\n",
      "AUC Score (Train): 0.974734\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=nag_sum/pos_sum,\n",
    " seed=27)\n",
    "modelfit(xgb1, x_train, y_train, x_test, y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "xgb_test = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=7,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.7,\n",
    " colsample_bytree=0.7,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=nag_sum/pos_sum,\n",
    " seed=27)\n",
    "\n",
    "# param_test1 = {\n",
    "#  'max_depth':range(3,10,2),\n",
    "#  'min_child_weight':range(1,6,2)\n",
    "# }\n",
    "# param_test3 = {\n",
    "#  'gamma':[i/10.0 for i in range(0,5)]\n",
    "# }\n",
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb_test, param_grid = param_test4 , scoring='roc_auc',n_jobs=4, cv=5)\n",
    "# gsearch1.fit(x_train,y_train)\n",
    "# gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nuo_nuaa\\anaconda3\\envs\\diveintoDL\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9165\n",
      "AUC Score (Train): 0.977014\n"
     ]
    }
   ],
   "source": [
    "xgb2 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=7,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.7,\n",
    " colsample_bytree=0.7,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=nag_sum/pos_sum,\n",
    " seed=27,\n",
    ")\n",
    "modelfit(xgb2, x_train, y_train, x_test, y_test)\n",
    "y_pred_xgb = xgb2.predict_proba(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:35:26] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nuo_nuaa\\anaconda3\\envs\\diveintoDL\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost准确率:  0.9113924050632911\n"
     ]
    }
   ],
   "source": [
    "# 5 xgboost\n",
    "# model_xgboost = XGBClassifier(learning_rate=0.01,\n",
    "#                       n_estimators=10,           # 树的个数-10棵树建立xgboost\n",
    "#                       max_depth=4,               # 树的深度\n",
    "#                       min_child_weight = 1,      # 叶子节点最小权重\n",
    "#                       gamma=0.,                  # 惩罚项中叶子结点个数前的参数\n",
    "#                       subsample=1,               # 所有样本建立决策树\n",
    "#                       colsample_btree=1,         # 所有特征建立决策树\n",
    "#                       scale_pos_weight=1,        # 解决样本个数不平衡的问题\n",
    "#                       random_state=27,           # 随机数\n",
    "#                       slient = 0\n",
    "#                       )\n",
    "\n",
    "# nag_sum = np.sum(y_train==0)\n",
    "# pos_sum = np.sum(y_train==1)\n",
    "model_xgboost = XGBClassifier(booster='gbtree', scale_pos_weight=nag_sum/pos_sum)\n",
    "model_xgboost.fit(x_train, y_train)\n",
    "# 用测试集做预测\n",
    "p_xgboost = model_xgboost.predict(x_test)\n",
    "print('xgboost准确率: ', metrics.accuracy_score(p_xgboost, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# # 测试结果\n",
    "# test_X_df = pd.read_excel(r\"D:\\GitHub\\modeling\\D\\ans3\\Molecular_Descriptor.xlsx\", engine='openpyxl',\n",
    "#                            sheet_name='test')\n",
    "# # test_X_df = test_X_df[idx_one]\n",
    "# test_X = test_X_df.iloc[:, 1:].values  # 去掉第一列\n",
    "# test_X = ss.fit_transform(test_X)\n",
    "# pred = model_xgboost.predict(test_X)\n",
    "# # list转dataframe\n",
    "# col_name = 'Caco-2'\n",
    "# df = pd.DataFrame(pred, columns=[col_name])\n",
    "# # 保存到本地excel\n",
    "# df.to_excel(col_name + \".xlsx\", index=False)\n",
    "\n",
    "# save model to file\n",
    "# pickle.dump(model_xgboost, open(col_name + \".pickle\", \"wb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机森林准确率:  0.9012658227848102\n"
     ]
    }
   ],
   "source": [
    "# 6 随机森林\n",
    "# model_rf = RandomForestClassifier(n_estimators = 1000, max_depth=None, random_state=0)\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(x_train, y_train)\n",
    "# 用测试集做预测\n",
    "p_rf = model_rf.predict(x_test)\n",
    "\n",
    "\n",
    "print('随机森林准确率: ', metrics.accuracy_score(p_rf, y_test))\n",
    "rf_pre = model_rf.predict_proba(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "确率:  0.9113924050632911\n"
     ]
    }
   ],
   "source": [
    "model_1 = pickle.load(open(r'D:\\GitHub\\modeling\\D\\ans3\\Caco-2.pickle', \"rb\"))\n",
    "pred_1 = model_1.predict(x_test)\n",
    "print('确率: ', metrics.accuracy_score(pred_1, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "# model_1 = pickle.load(open(r'D:\\GitHub\\modeling\\D\\ans3\\Caco-2.pickle', \"rb\"))\n",
    "# pred_2 = model_1.predict(x_train)\n",
    "# print('确率: ', metrics.accuracy_score(pred_2, y_train))\n",
    "# auc_saved_path = 'p_xgboost.pkl'\n",
    "# fpr, tpr, threshold = roc_curve(p_xgboost, y_test)  ###计算真正率和假正率\n",
    "# roc_auc = auc(fpr, tpr)  ###计算auc的值\n",
    "# print(fpr.shape, tpr.shape, threshold.shape)\n",
    "# print('fpr= ', fpr)\n",
    "# print('tpr= ', tpr)\n",
    "# print('threshold= ', threshold)\n",
    "# data_roc = dict()\n",
    "# data_roc['micro-auc'] = roc_auc\n",
    "# data_roc['fpr-micro'] = fpr\n",
    "# data_roc['tpr-micro'] = tpr\n",
    "#\n",
    "# output = open(auc_saved_path, 'wb')\n",
    "# pickle.dump(data_roc, output)\n",
    "# output.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "# num_class =2\n",
    "# fpr_dict = dict()\n",
    "# tpr_dict = dict()\n",
    "# roc_auc_dict = dict()\n",
    "# auc_saved_path = 'p_xgboost.pkl'\n",
    "# # micro\n",
    "# fpr_dict[\"micro\"], tpr_dict[\"micro\"], _ = roc_curve(p_xgboost, y_test)\n",
    "# roc_auc_dict[\"micro\"] = auc(fpr_dict[\"micro\"], tpr_dict[\"micro\"])\n",
    "#\n",
    "# # macro\n",
    "# # First aggregate all false positive rates\n",
    "# all_fpr = np.unique(np.concatenate([fpr_dict[i] for i in range(num_class)]))\n",
    "# # Then interpolate all ROC curves at this points\n",
    "# mean_tpr = np.zeros_like(all_fpr)\n",
    "# for i in range(num_class):\n",
    "#     mean_tpr += interp(all_fpr, fpr_dict[i], tpr_dict[i])\n",
    "# # Finally average it and compute AUC\n",
    "# mean_tpr /= num_class\n",
    "# fpr_dict[\"macro\"] = all_fpr\n",
    "# tpr_dict[\"macro\"] = mean_tpr\n",
    "# roc_auc_dict[\"macro\"] = auc(fpr_dict[\"macro\"], tpr_dict[\"macro\"])\n",
    "# print('micro-auc: ', roc_auc_dict[\"micro\"])\n",
    "# print('macro-auc: ', roc_auc_dict[\"macro\"])\n",
    "# print('0-auc: ', roc_auc_dict[0])\n",
    "# print('1-auc: ', roc_auc_dict[1])\n",
    "#\n",
    "# data_roc = dict()\n",
    "# data_roc['0-auc'] = roc_auc_dict[0]\n",
    "# data_roc['1-auc'] = roc_auc_dict[1]\n",
    "# data_roc['micro-auc'] = roc_auc_dict[\"micro\"]\n",
    "# data_roc[\"macro-auc\"] = roc_auc_dict[\"macro\"]\n",
    "# data_roc['fpr-micro'] = fpr_dict[\"micro\"]\n",
    "# data_roc['tpr-micro'] = tpr_dict[\"micro\"]\n",
    "# data_roc['fpr-macro'] = fpr_dict[\"macro\"]\n",
    "# data_roc['tpr-macro'] = tpr_dict[\"macro\"]\n",
    "#\n",
    "# output = open(auc_saved_path, 'wb')\n",
    "# pickle.dump(data_roc, output)\n",
    "# output.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-auc:  0.9689472840890881\n",
      "macro-auc:  0.967691233874891\n",
      "0-auc:  0.9671843149876622\n",
      "1-auc:  0.9671843149876623\n"
     ]
    }
   ],
   "source": [
    "# score_list = test_predict_label  # svm\n",
    "# score_list = y_pred_xgb  # xgb\n",
    "score_list = rf_pre  # xgb\n",
    "\n",
    "label_list = y_test\n",
    "num_class = 2\n",
    "auc_saved_path = 'p_rf.pkl'\n",
    "# 统计roc相关\n",
    "score_array = np.array(score_list)\n",
    "# 将label转换成onehot形式\n",
    "label_tensor = torch.tensor(label_list)\n",
    "label_tensor = label_tensor.reshape((label_tensor.shape[0], 1))\n",
    "label_onehot = torch.zeros(label_tensor.shape[0], num_class)\n",
    "label_onehot.scatter_(dim=1, index=label_tensor, value=1)\n",
    "label_onehot = np.array(label_onehot)\n",
    "# 调用sklearn库，计算每个类别对应的fpr和tpr\n",
    "num_class =2\n",
    "fpr_dict = dict()\n",
    "tpr_dict = dict()\n",
    "roc_auc_dict = dict()\n",
    "for i in range(num_class):\n",
    "    fpr_dict[i], tpr_dict[i], _ = roc_curve(label_onehot[:, i], score_array[:, i])\n",
    "    roc_auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])\n",
    "# micro\n",
    "fpr_dict[\"micro\"], tpr_dict[\"micro\"], _ = roc_curve(label_onehot.ravel(), score_array.ravel())\n",
    "roc_auc_dict[\"micro\"] = auc(fpr_dict[\"micro\"], tpr_dict[\"micro\"])\n",
    "\n",
    "# macro\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr_dict[i] for i in range(num_class)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(num_class):\n",
    "    mean_tpr += interp(all_fpr, fpr_dict[i], tpr_dict[i])\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= num_class\n",
    "fpr_dict[\"macro\"] = all_fpr\n",
    "tpr_dict[\"macro\"] = mean_tpr\n",
    "roc_auc_dict[\"macro\"] = auc(fpr_dict[\"macro\"], tpr_dict[\"macro\"])\n",
    "print('micro-auc: ', roc_auc_dict[\"micro\"])\n",
    "print('macro-auc: ', roc_auc_dict[\"macro\"])\n",
    "print('0-auc: ', roc_auc_dict[0])\n",
    "print('1-auc: ', roc_auc_dict[1])\n",
    "\n",
    "data_roc = dict()\n",
    "data_roc['0-auc'] = roc_auc_dict[0]\n",
    "data_roc['1-auc'] = roc_auc_dict[1]\n",
    "data_roc['micro-auc'] = roc_auc_dict[\"micro\"]\n",
    "data_roc[\"macro-auc\"] = roc_auc_dict[\"macro\"]\n",
    "data_roc['fpr-micro'] = fpr_dict[\"micro\"]\n",
    "data_roc['tpr-micro'] = tpr_dict[\"micro\"]\n",
    "data_roc['fpr-macro'] = fpr_dict[\"macro\"]\n",
    "data_roc['tpr-macro'] = tpr_dict[\"macro\"]\n",
    "\n",
    "output = open(auc_saved_path, 'wb')\n",
    "pickle.dump(data_roc, output)\n",
    "output.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}